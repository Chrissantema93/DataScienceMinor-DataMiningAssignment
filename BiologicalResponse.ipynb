{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "data_path = ['data']\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the data using the file path\n",
    "filepathTrain = os.sep.join(['train.csv'])\n",
    "trainData = pd.read_csv(filepathTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#getting the target and the trainings data\n",
    "x = trainData.iloc[:,trainData.columns != 'Activity']  \n",
    "y = trainData.iloc[:,0]    #target column\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tree classifier to determine the most impactful features as to not having to use all features/overfitting\n",
    "#because trees determine the highest impact of a feature it looked as a good choice to use to determine the most impactful\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x,y)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=x.columns)\n",
    "results = feat_importances.nlargest(500)\n",
    "#print(results.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8056516706489172\n"
     ]
    }
   ],
   "source": [
    "#select the top important features\n",
    "trainDataFeats = trainData[results.index]\n",
    "\n",
    "from sklearn import model_selection\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#iniatializing the randomforest with mostly defaults except n_estimators at 1000 and random state for reproducibility\n",
    "clf = RandomForestClassifier(1000, random_state = random.seed(26))\n",
    "\n",
    "#another way to measure the accuracy with cross validation\n",
    "print(np.mean(cross_val_score(clf, trainDataFeats, y, cv=10)))\n",
    "\n",
    "\n",
    "#fitting and predicting of the KNN algorithm\n",
    "#knn = knn.fit(trainDataFeats, y)\n",
    "#y_pred = knn.predict(trainDataFeats)\n",
    "\n",
    "#fitting and predicting of the random forest\n",
    "clf = clf.fit(trainDataFeats, y)\n",
    "y_predclf = clf.predict(trainDataFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is code to test different input parameters for the random forest algorithm. \n",
    "#i commented this because it takes more than 30 mins to run but you're welcome to uncomment and run\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#param_grid = {\n",
    "               # 'criterion' : [\"gini\", \"entropy\"],\n",
    "                 #'n_estimators': [1,10,100,500, 1000],\n",
    "                # 'max_depth': [7, 9],\n",
    "                #'min_samples_split': [2,10],\n",
    "                #'max_features' : ['auto','log2' ]\n",
    "             #}\n",
    "\n",
    "\n",
    "#grid_clf = GridSearchCV(clf, param_grid, cv=10)\n",
    "#grid_clf.fit(trainDataFeats, y)\n",
    "\n",
    "#print(grid_clf.best_estimator_)\n",
    "#print(grid_clf.best_params_)\n",
    "#print(grid_clf.cv_results_)\n",
    "\n",
    "\n",
    "# above gridsearchcv resulted in the following results when i ran it\n",
    "#'criterion': 'entropy', 'max_depth': 9, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 1000}\n",
    "\n",
    "\n",
    "#using mostly defaults though except n_estimators 1000 still give best results as can been seen above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#accuracy of the predictions, stays on 1.0 mostly with the random forest, sounds like overfitting still..\n",
    "def accuracy(real, predict):\n",
    "    return sum(y == y_predclf) / float(real.shape[0])\n",
    "print(accuracy(y, y_predclf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#knn Probability, old code which i started with which uses the KNN algorithm but this resulted in scores of like 6 on kaggle\n",
    "#and thus was not sufficient enough to use..\n",
    "#filepathTest = os.sep.join(['test.csv'])\n",
    "#testDataProb = pd.read_csv(filepathTest)\n",
    "#testDataFeatsProb = testDataProb[results.index]\n",
    "\n",
    "\n",
    "#y_prob2 = knn.predict_proba(testDataFeatsProb)[:,1]\n",
    "#testDataProb.insert(0, \"Probability\", y_prob2)\n",
    "#resultsdf = pd.DataFrame()\n",
    "#resultsdf.insert(0, \"PredictedProbability\", y_prob2clf) \n",
    "#resultsdf.insert(0, \"MoleculeId\", range(1,2502))\n",
    "#export_csv = resultsdf.to_csv (r'biologicalResponseknn.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest probability results\n",
    "#ive read that the random forest tree is a good algorithm because it gets rid of the variance introduced by some very\n",
    "#high impacting features and because i've already chosen the top 500 features to reduce overfitting\n",
    "#it seemed like a good idea to use and so far the results have been way better than KNN\n",
    "#u can find some example knn predicting in the document, this resulted in a kaggle score of 6+\n",
    "\n",
    "filepathTest = os.sep.join(['test.csv'])\n",
    "testDataProbClf = pd.read_csv(filepathTest)\n",
    "testDataFeatsClf = testDataProbClf[results.index]\n",
    "\n",
    "#predict the test data just using the probabilities for an 1 or true (hence the :,1) because we dont care about both chances\n",
    "y_prob2clf = clf.predict_proba(testDataFeatsClf)[:,1]\n",
    "#make new dataframe and fill it with the data and export it as csv for kaggle\n",
    "resultsdf = pd.DataFrame()\n",
    "resultsdf.insert(0, \"PredictedProbability\", y_prob2clf) \n",
    "resultsdf.insert(0, \"MoleculeId\", range(1,2502))\n",
    "export_csv = resultsdf.to_csv (r'biologicalResponseRF.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
